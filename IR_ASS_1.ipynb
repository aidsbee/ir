{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DyKwnpsQ5rc"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "from  nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello I am Gayatri Deshmukh. I am from Nanded District. I will be an Engineer in few months.\"\n",
        "\n",
        "#Tokenization\n",
        "nltk.download('punkt')\n",
        "tokenized_words = word_tokenize(sentence)\n",
        "tokenized_sentences = sent_tokenize(sentence)\n",
        "\n",
        "print(tokenized_words)\n",
        "print(tokenized_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBb8EzoERcbO",
        "outputId": "f939b57e-d6f2-4ebd-b7bb-bca3f2160c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months', '.']\n",
            "['Hello I am Gayatri Deshmukh.', 'I am from Nanded District.', 'I will be an Engineer in few months.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop words removal\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "cleaned_token = []\n",
        "for i in tokenized_words:\n",
        "  if i not in stop_words:\n",
        "    cleaned_token.append(i)\n",
        "\n",
        "print(\"Unclean version \", tokenized_words)\n",
        "print(\"Clean Version\", cleaned_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcavAISFRgDk",
        "outputId": "a3efc9d1-0533-40d4-fcc1-1feb5e976fd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unclean version  ['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months', '.']\n",
            "Clean Version ['Hello', 'I', 'Gayatri', 'Deshmukh', '.', 'I', 'Nanded', 'District', '.', 'I', 'Engineer', 'months', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "\n",
        "stemmed_words = []\n",
        "for i in tokenized_words:\n",
        "    stemmed = snowball_stemmer.stem(i)\n",
        "    stemmed_words.append(stemmed)\n",
        "\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsRLD7HTRhyO",
        "outputId": "771793e4-2d07-4ab0-c9d6-015918bd070f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'i', 'am', 'gayatri', 'deshmukh', '.', 'i', 'am', 'from', 'nand', 'district', '.', 'i', 'will', 'be', 'an', 'engin', 'in', 'few', 'month', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_words = []\n",
        "for i in tokenized_words:\n",
        "    lemmatized = wordnet_lemmatizer.lemmatize(i)\n",
        "    lemmatized_words.append(lemmatized)\n",
        "\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF3qBcxTRogU",
        "outputId": "c7ca10d3-b878-4c13-be30-4da0e0d4a945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'month', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Pos Tagging\n",
        "# dt - determinnant\n",
        "# NN - noun\n",
        "# In - prep / conjunc\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tag = nltk.pos_tag(tokenized_words)\n",
        "\n",
        "print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYF76i_LRsVP",
        "outputId": "d67ed4b4-6784-4143-daa2-e99f911e2779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'NNP'), ('I', 'PRP'), ('am', 'VBP'), ('Gayatri', 'NNP'), ('Deshmukh', 'NNP'), ('.', '.'), ('I', 'PRP'), ('am', 'VBP'), ('from', 'IN'), ('Nanded', 'NNP'), ('District', 'NNP'), ('.', '.'), ('I', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('an', 'DT'), ('Engineer', 'NNP'), ('in', 'IN'), ('few', 'JJ'), ('months', 'NNS'), ('.', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d0 = \"Good Morning\"\n",
        "d1 = \"Do daily exercise in the morning \"\n",
        "d2 = \"exercise is good for health\"\n",
        "series = [d0, d1, d2]\n",
        "tfidf = TfidfVectorizer()\n",
        "result = tfidf.fit_transform(series)\n",
        "\n",
        "print(\"Word Indexing: \", tfidf.vocabulary_)\n",
        "print(\"tf-idf in matrix form: \\n\", result.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg4pEZnURvsR",
        "outputId": "e2cc42b5-5fea-4fb5-924d-5a19fd8e980b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Indexing:  {'good': 4, 'morning': 8, 'do': 1, 'daily': 0, 'exercise': 2, 'in': 6, 'the': 9, 'is': 7, 'for': 3, 'health': 5}\n",
            "tf-idf in matrix form: \n",
            " [[0.         0.         0.         0.         0.70710678 0.\n",
            "  0.         0.         0.70710678 0.        ]\n",
            " [0.44036207 0.44036207 0.3349067  0.         0.         0.\n",
            "  0.44036207 0.         0.3349067  0.44036207]\n",
            " [0.         0.         0.37302199 0.49047908 0.37302199 0.49047908\n",
            "  0.         0.49047908 0.         0.        ]]\n"
          ]
        }
      ]
    }
  ]
}